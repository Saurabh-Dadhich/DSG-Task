{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amino-melbourne",
   "metadata": {},
   "source": [
    "## Saurabh Dadhich\n",
    "### Indian Institute of Technology, Roorkee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-birthday",
   "metadata": {},
   "source": [
    "## Blood Cell Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-trader",
   "metadata": {},
   "source": [
    "### Importing required packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "touched-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.models import Model \n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam   \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-importance",
   "metadata": {},
   "source": [
    "### Data Preprocessing:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subject-samoa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_image[0][0] before preprocess_input function:  [208. 213. 209.]\n",
      "Test_image[0][0] after preprocess_input function:  [105.061  96.221  84.32 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATDElEQVR4nO3df7BcdX3G8feTEEL5HSBgDEkTmMDwQw30Gmuj1EoVyHQMdEZNpjqpMsYfUHHEaQMOFu0wVSvQcSpokEyjIhCLSKZNW9OMo9WqcIMhEELMTQhySUwCKKRJiMnNp3+cc8vmZm/uZvecnN39Pq+Znd397jlnPyebPDl7ztnzUURgZukaVXUBZlYth4BZ4hwCZolzCJglziFgljiHgFniSgsBSZdLWiepT9KCst7HzFqjMs4TkDQa+CXwDqAfeASYGxFPFv5mZtaSsrYEZgB9EbExIn4H3AfMLum9zKwFR5W03InAszXP+4E3DTfxaaedFlOmTCmpFDMDWLly5fMRMX7oeFkhoDpjB3zvkDQfmA8wefJkent7i61gYOg75kbXVLevZvxYYG8+sPeo8v5kzCoi6Zl642V9HegHJtU8PxPYXDtBRCyMiJ6I6Bk//qBwat37gTHAmL0wZk9+2w9rgD357YzBaYC9e4Cx2ZMxe7LXzRJQVgg8AkyTNFXS0cAcYGlJ73WwPXtgYCewE5gFHJPfeuF1O+GY/dnTF3fl0+wETgD25ws4Jtsy2HXEKjarTCkbvRGxT9K1wH+SbYAviog1ZbzXQXbvho9+FJYsrvPi4G6J1cBkYCrwm/rL2f8SnAQ8D5x0UgmFmrWH0r75RsQyYFlZy69r50745Cdhcb0AqPX6BhZ2crbP4DXHZMFi1qW654zBHbvhxptg4cJilxvAlmIXadZOuiMEdgK3fA++/FDxy94DnFP8Ys3aRecfCPstsAhYvBHYWG0tZh2os7cEXgS+BFz/Evx6mB18LQvY/ww8XtLizSrW2SHwb8AtkP0LfaSkN9kHuxbBH5e0eLOKdXYI/L+3AO8sadljgM+WtGyz6nVJCJhZsxwCIxoASjjqYNaMnzPkBPzWdVEIvA64sMDlDf704bXAvxa4XLMW9AK/LnaRXRQC78pvRTk1v38NcFeByzVrwTXAxcBPgBeKWWQXhUDRVuX3j1VZhFl9X6Ww02IcAo16BfjHqoswy70dKOgX+A6BRu0G/r7qIsxyHwCmFLOozg6BHuCDwLjBgVnAJQW/yV7geuAe2IFPGbCu09khcB5wE/AV4GvA12bCpRcV/CYDwO3AZ2H3p7LfKZh1kaZDQNIkST+QtFbSGknX5eM3S3pO0qr8Nqu4cuuYAswlu1rhfODC9wHfBN5c4JsEsD5frll3aeVXhPuA6yPiUUknACslLc9fuz0ivtR6eU34YA882QPLz+XVCx7fBAzX8uBbwO8B7+XAK4+apaHpEIiILeSX24iIHZLWkl1qvFqvB24D/uqNsP6N2U+Nd04k+7nhMg68cOAM4ErgOGAm8MNDL/tkYKSLFpl1mEL2CUiaAlxEdlIjwLWSVktaJGnc8HOW5ELgDuBBsqscvuFNwBc4OKP6ePXiondwyD+OUSfA674Hbyu2VLOqtRwCko4HHgA+EREvA3cCZwPTybYUbh1mvvmSeiX1bt++vdUyDnYe8Eay/+BPAvgc2ZXQa71I9q96BjCN+u0ScscfBV8vcj+DWXtoKQQkjSELgHsi4rsAEbE1IgYiYj/Z+bYz6s1bet+BgzxJdrB/qEfJrkUwXE/GUXDsLPif//ZlxqwrtXJ0QMDdwNqIuK1mfELNZFcBTzRfXouuBc4FHnkfjV0aqM7Ow7FjYNXX4YILiq3NrE20cnRgJlmfn8clrcrHbgTmSppO9l/rJuDDLbxH8z5Odkx/N2RHCV5pYKZzgKfzx1OzOwmmTRhuBrOO18rRgR9T/0v0ke01MNRdwJeBvo/BKw/mg4fxc6uLpwABj27Ovsg8dIj9BGZdoPOvNjzUU2T9BuO3HPYPr7eStSNEsG9C9qfj5kPW5bovBAaA+DjwncOf9xS68U/E7JC666/8zcBX/prsmP9AtbWYdYjO/gHRUHuBfb+jqQDYQdY61Swx3RUCf0d2VOCwvQxHjznkuUJm3aq7QmAU2dV/PnSY8/3vaBjjBLA0dVcIAOh2+NqH4DNkO/pG8tJLcNyx3gqwZHVfCCDQQvhswAsBcQV8bBScOApGjcrWeFT++Pnn4cQTqy7YrFJdGAJDLYOvDMBLAzAwAAPnw8DG7PGpp448u1mX665DhA1ZU3UBZm0lgS0BMzsUh4BZ4hwCZolzCJglziFgljiHgFniWjpEKGkT2U9vBoB9EdEj6RTgfrK2IJuA90TEb1or08zKUsSWwJ9ExPSI6MmfLwBWRMQ0YEX+3MzaVBlfB2bzaouOxWTdPcysTbUaAgF8X9JKSfPzsTPy7kSDXYpOrzdj6X0HzKwhrZ42PDMiNks6HVgu6alGZ4yIhcBCgJ6enuEu+m9mJWtpSyAiNuf328iafs0Atg72Hsjvt7VapJmVp5XmI8fl3YiRdBzwTrJGI0uBeflk84CHWi3SzMrTyteBM4AHs0ZEHAV8OyL+Q9IjwBJJVwO/At7deplmVpZWmo9sBN5QZ/wF4NJWijKzI8dnDJolziFgljiHgFniHAJmiXMImCXOIWCWOIdAN3gO+KP89uGKa7GOk+Alx7vQK8BP88fupGSHyVsC3eQ84FtVF2GdxlsC3WAy0AeMBc6suBbrOA6BbjAGOLvqIqxT+euAWeIcAmaJcwhYY04F9lZdhJXBIWCN2VV1AVaWpncMSjqXrL/AoLOAzwAnAx8CBq8eemNELGv2faxN7ABGV12ElaGVi4qsA6YDSBpNdt7ag8AHgNsj4ktFFGhtwseRulZRXwcuBTZExDMFLc/MjpCiQmAOcG/N82slrZa0SNK4gt7DzErQcghIOhp4F/CdfOhOslNXpgNbgFuHmc/NR8zaQBFbAlcAj0bEVoCI2BoRAxGxH7iLrBfBQSJiYUT0RETP+PHjCyjDzJpRRAjMpearwGDjkdxVZL0IzKxNtdqa/FjgHRz4K/YvSppO1qdwE/6Fu1lbaykEImIX2blktWPvb6kiMzuifMagWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJW7EEMgvFrpN0hM1Y6dIWi5pfX4/rua1GyT1SVon6bKyCjezYjSyJfDPwOVDxhYAKyJiGrAif46k88muPHxBPs8deU8CM2tTI4ZARPwIeHHI8Gxgcf54MXBlzfh9EbEnIp4G+hjmQqNm1h6a3SdwRkRsAcjvT8/HJwLP1kzXn4+ZWZsqeseg6oxF3Qndd8CsLTQbAlsHLy2e32/Lx/uBSTXTnQlsrrcA9x0waw/NhsBSYF7+eB7wUM34HEljJU0FpgEPt1aimZVpxEuOS7oXeBtwmqR+4G+BzwNLJF0N/Ap4N0BErJG0BHgS2AdcExEDJdVuZgUYMQQiYu4wL106zPS3ALe0UpSZHTk+Y9AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEvciD8gMrMm1F5F47WVVdEQh4BZ0QKYCWwBBoC91ZYzEn8dKNJO4BkOviyrpUXARrLL7p5bcS0NcAgUaTVwE7Cs6kKscgLuB54YacLqNdt85B8kPSVptaQHJZ2cj0+RtFvSqvz21RJrbz9vBr4BvK/qQswa12zzkeXAhRHxeuCXwA01r22IiOn57SPFlGlmZWmq+UhEfD8i9uVPf0Z2VWEz60BF7BP4IPDvNc+nSvqFpB9KeutwM7nvgFl7aCkEJH2a7KrC9+RDW4DJEXER8Eng25JOrDev+w6YtYemQ0DSPODPgL+IiADIexC+kD9eCWwAzimiUDMrR1MhIOly4G+Ad0XErprx8YNdiCWdRdZ8ZGMRhZpZOZptPnIDMBZYLgngZ/mRgEuAz0naR3au1EciwqfOmLWxZpuP3D3MtA8AD7RalJkdOT5j0KybLAReObxZHAJm3WQyh/2v2r8iNOsmQ8/tbYC3BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q123fgZknP1fQXmFXz2g2S+iStk3RZWYWbWTGa7TsAcHtNf4FlAJLOB+YAF+Tz3DF4uTEza09N9R04hNnAffkFR58G+oAZLdRnZiVrZZ/AtXkbskWSxuVjE4Fna6bpz8cO4r4DZu2h2RC4EzgbmE7Wa+DWfFx1po16C3DfAbP20FQIRMTWiBiIiP3AXby6yd8PTKqZ9Exgc2slmlmZmu07MKHm6VW82oB5KTBH0lhJU8n6DjzcWolmVqZm+w68TdJ0sk39TcCHASJijaQlwJNk7cmuiYiBUio3s0Io7yBWqZ6enujt7a26DLOuJmllRPQMHfcZg2aJcwiYJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJc4hYJa4ZvsO3F/Tc2CTpFX5+BRJu2te+2qJtZtZAUa8shBZ34F/Ar4xOBAR7x18LOlW4KWa6TdExPSC6jOzko0YAhHxI0lT6r0mScB7gLcXXJeZHSGt7hN4K7A1ItbXjE2V9AtJP5T01haXb2Yla+TrwKHMBe6teb4FmBwRL0j6A+B7ki6IiJeHzihpPjAfYPLkyS2WYWbNanpLQNJRwJ8D9w+O5e3HXsgfrwQ2AOfUm9/NR8zaQytfB/4UeCoi+gcHJI0fbEAq6SyyvgMbWyvRzMrUyCHCe4GfAudK6pd0df7SHA78KgBwCbBa0mPAvwAfiYhGm5maWQUaOTowd5jxv6wz9gDwQOtlmdmR4jMGzRLXnSFwK/BfVRdh1hlaPUTYnq6jW+PNrHDdGQLduVZmpfD/l2aJcwiYJc4hYJY4h4BZ4hwCZolLKwR2AfurLsKsvaQVAu/AP2cyGyKtI+o/qboAs/aT1paAmR3EIWCWOIeAWeIauajIJEk/kLRW0hpJ1+Xjp0haLml9fj+uZp4bJPVJWifpsjJXwMxa08iWwD7g+og4D/hD4BpJ5wMLgBURMQ1YkT8nf20OcAFwOXDH4CXHzKz9jBgCEbElIh7NH+8A1gITgdnA4nyyxcCV+ePZwH35RUefBvqAGQXXbWYFOax9AnkTkouAnwNnRMQWyIICOD2fbCLwbM1s/fmYmbWhhkNA0vFk1w/8RL0+ArWT1hmLOsubL6lXUu/27dsbLcPMCtZQCEgaQxYA90TEd/PhrZIm5K9PALbl4/3ApJrZzwQ2D12m+w6YtYdGjg4IuBtYGxG31by0FJiXP54HPFQzPkfSWElTyXoPPFxcyWZWpEZOG54JvB94fLAFOXAj8HlgSd6H4FfAuwEiYo2kJcCTZEcWromIgaILN7NiNNJ34MfU/54PcOkw89wC3NJCXWZ2hPiMQbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwSp4iDrgZ+5IuQtgM7geerrqUFp9HZ9UPnr0On1w/lrsPvR8RBl/ZuixAAkNQbET1V19GsTq8fOn8dOr1+qGYd/HXALHEOAbPEtVMILKy6gBZ1ev3Q+evQ6fVDBevQNvsEzKwa7bQlYGYVqDwEJF0uaZ2kPkkLqq6nUZI2SXpc0ipJvfnYKZKWS1qf34+rus5BkhZJ2ibpiZqxYeuVdEP+mayTdFk1VR9omHW4WdJz+eewStKsmtfaah0kTZL0A0lrJa2RdF0+Xu3nEBGV3YDRwAbgLOBo4DHg/CprOozaNwGnDRn7IrAgf7wA+ELVddbUdglwMfDESPUC5+efxVhgav4ZjW7TdbgZ+FSdadtuHYAJwMX54xOAX+Z1Vvo5VL0lMAPoi4iNEfE74D5gdsU1tWI2sDh/vBi4srpSDhQRPwJeHDI8XL2zgfsiYk9EPA30kX1WlRpmHYbTdusQEVsi4tH88Q5gLTCRij+HqkNgIvBszfP+fKwTBPB9SSslzc/HzoiILZB94MDplVXXmOHq7bTP5VpJq/OvC4Ob0m29DpKmABcBP6fiz6HqEKjX7bhTDlfMjIiLgSuAayRdUnVBBeqkz+VO4GxgOrAFuDUfb9t1kHQ88ADwiYh4+VCT1hkrfB2qDoF+YFLN8zOBzRXVclgiYnN+vw14kGwzbaukCQD5/bbqKmzIcPV2zOcSEVsjYiAi9gN38ermcluug6QxZAFwT0R8Nx+u9HOoOgQeAaZJmirpaGAOsLTimkYk6ThJJww+Bt4JPEFW+7x8snnAQ9VU2LDh6l0KzJE0VtJUYBrwcAX1jWjwH0/uKrLPAdpwHSQJuBtYGxG31bxU7efQBnt8Z5HtJd0AfLrqehqs+SyyvbaPAWsG6wZOBVYA6/P7U6qutabme8k2l/eS/Q9z9aHqBT6dfybrgCuqrv8Q6/BN4HFgdf6PZkK7rgPwFrLN+dXAqvw2q+rPwWcMmiWu6q8DZlYxh4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXu/wDdcS8YUD8WrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image_path = r'F:\\DSG_IITR\\Blood cell\\dataset2-master\\dataset2-master\\images\\TEST_SIMPLE\\EOSINOPHIL\\_0_5239.jpeg'\n",
    "test_image = image.load_img(test_image_path,target_size=(224,224))\n",
    "test_image = image.img_to_array(test_image)\n",
    "print(\"Test_image[0][0] before preprocess_input function: \",test_image[0][0]) \n",
    "test_image = preprocess_input(test_image)\n",
    "print(\"Test_image[0][0] after preprocess_input function: \",test_image[0][0])\n",
    "plt.imshow(test_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-baghdad",
   "metadata": {},
   "source": [
    "#### Creating data and Finding Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abroad-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder in os.listdir(data_path):\n",
    "        if folder =='EOSINOPHIL': \n",
    "            label = 0\n",
    "        elif folder == 'LYMPHOCYTE': \n",
    "            label = 1\n",
    "        elif folder == 'MONOCYTE': \n",
    "            label = 2\n",
    "        elif folder == 'NEUTROPHIL': \n",
    "            label = 3\n",
    "        for file in os.listdir(os.path.join(data_path, folder)):\n",
    "            image_path    =  os.path.join(os.path.join(data_path,folder),file)\n",
    "            img           =  image.load_img(image_path,target_size=(224,224))\n",
    "            img           =  image.img_to_array(img)\n",
    "            img           =  preprocess_input(img)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "            \n",
    "    images = np.array(images, dtype = 'float32')\n",
    "    labels = np.array(labels, dtype = 'int32')\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-olympus",
   "metadata": {},
   "source": [
    "### Train and Test folder path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decimal-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"dataset2-master\\dataset2-master\\images\\TRAIN\"\n",
    "test_path  = r\"dataset2-master\\dataset2-master\\images\\TEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-surfing",
   "metadata": {},
   "source": [
    "### Loading Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mechanical-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X,train_data_Y = data_loading(train_path)\n",
    "test_data_X,test_data_Y = data_loading(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-arthur",
   "metadata": {},
   "source": [
    "### Shuffling data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considered-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X ,train_data_Y = shuffle(train_data_X,train_data_Y, random_state=2)\n",
    "test_data_X ,test_data_Y = shuffle(test_data_X,test_data_Y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-commercial",
   "metadata": {},
   "source": [
    "### One-hot encoding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "announced-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data_Y shape:   (9957, 4)\n",
      "Train_data_Y[0]:\n",
      " [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "lb = LabelBinarizer()                 # For labels\n",
    "train_y = lb.fit_transform(train_data_Y)\n",
    "print(\"Train_data_Y shape:  \",train_y.shape) \n",
    "print(\"Train_data_Y[0]:\\n\",train_y[0])  \n",
    "test_y = lb.fit_transform(test_data_Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-station",
   "metadata": {},
   "source": [
    "### Train-Validation Split: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lightweight-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, val_X, train_Y, val_Y) = train_test_split(train_data_X,train_y,test_size=0.20,shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "objective-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X,test_Y = test_data_X,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "supreme-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train_X:   (7965, 224, 224, 3)\n",
      "Shape of Train_Y:   (7965, 4)\n",
      "Shape of Val_X:     (1992, 224, 224, 3)\n",
      "Shape of Val_Y:     (1992, 4)\n",
      "Shape of Test_X:    (2487, 224, 224, 3)\n",
      "Shape of Test_Y:    (2487, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Train_X:  \",train_X.shape)\n",
    "print(\"Shape of Train_Y:  \",train_Y.shape)\n",
    "print(\"Shape of Val_X:    \",val_X.shape)\n",
    "print(\"Shape of Val_Y:    \",val_Y.shape)\n",
    "print(\"Shape of Test_X:   \",test_X.shape)\n",
    "print(\"Shape of Test_Y:   \",test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-writing",
   "metadata": {},
   "source": [
    "### Model architecture: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-password",
   "metadata": {},
   "source": [
    "#### Transfer Learning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "electoral-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top = False,weights = 'imagenet', input_shape = train_X.shape[1:]) \n",
    "# include_top = false     ---> to remove last layers to use transfer learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expensive-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "invalid-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = Flatten()(base_model.output)\n",
    "head_model = Dropout(0.4)(head_model) \n",
    "head_model = Dense(256)(head_model)\n",
    "head_model = BatchNormalization()(head_model)\n",
    "head_model = Activation('relu')(head_model)\n",
    "head_model = Dropout(0.4)(head_model)\n",
    "head_model = Dense(128)(head_model)\n",
    "head_model = BatchNormalization()(head_model)\n",
    "head_model = Activation('relu')(head_model)\n",
    "head_model = Dropout(0.3)(head_model) \n",
    "head_model = Dense(4,activation = 'softmax')(head_model)\n",
    "\n",
    "\n",
    "model = Model(inputs = base_model.input, outputs = head_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "brutal-plant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,172,420\n",
      "Trainable params: 6,456,964\n",
      "Non-trainable params: 14,715,456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chief-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-equipment",
   "metadata": {},
   "source": [
    "### Fitting model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7965 samples, validate on 1992 samples\n"
     ]
    }
   ],
   "source": [
    "his = model.fit(train_X, train_Y, batch_size = 32, epochs =30,validation_data = (val_X, val_Y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,acc = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacee7e",
   "metadata": {},
   "source": [
    "### Getting \"Kernel Died\" while running model.fit() ,  tried many ways to deal with this problem but unable to find a solution but this is how I will approach the project (Blood cell Classification).    \n",
    "##### I was using anaconda - Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c1a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
